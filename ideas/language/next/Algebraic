Algebraic Data Abstraction
==========================
Algebraic Data Abstraction is a new data abstraction mechanism based
on the idea that abstract data types are algebras.
ADA is (1) modular, and supports (2) generic programming, with (3) run-time
polymorphism (including overloaded binary operators and heterogenous lists).
It solves the "expression problem", which is actually a modularity problem.

Why is it needed? Because nothing else meets all 3 requirements.
Existing data abstraction mechanisms, such as Object Oriented Programming,
either support run-time polymorphism but are unmodular,
or they are modular but do not support run-time polymorphism.

Data abstraction is needed at Curv level 4 (library implementation) so that
we can reduce complexity by creating simple powerful abstractions that hide
their implementation.

Algebras & Theories
-------------------
An abstract data type (ADT) is a set of values (a Type), plus a set of
operations on values of that type, plus a set of axioms (or laws) which
specify the semantics of the operations. An ADT is abstract because when we
use an ADT to write code, we ignore the representation of the type, and we
only use the operations.

This structure (a type, a set of named operations, and a set of axioms)
is a single-typed Algebra. There are also multi-typed algebras.

Since an Algebra may contain multiple types, each type has a name.
By convention, a single-typed algebra has a type named T.

The named types and operations in an Algebra are collectively called members.
In Curv, an Algebra `A` is a module value, and you use dot notation,
`A.T` and `A.op`, to reference the members of an Algebra.

In an Algebra, an operation is a named value, and typically fits into
one of the following categories:
* It is a constant belonging to one of the Algebra's member types.
* It is a function of one or more arguments that returns an instance
  of one of the member types.
* It is a function that takes one or more arguments that are instances
  of a member type, and may return an instance of a member type.

A Theory is the type of an Algebra. A Theory comprises a set of type names,
a set of operation type signatures, and a set of axioms.

Two Theories may be related by a subtheory/supertheory relationship.
A subtheory contains all of the type names, operation signatures and axioms
of its supertheory, and adds additional ones.

This is a very general framework.
You can have two algebras that implement the same theory
over the same type, but the operations are implemented differently.
An example from mathematics is that there is both an additive and
a multiplicative monoid over the real numbers.

Generic Programming
-------------------
The fundamental benefit of data abstraction is modularity. It allows you
to separate the API for constructing and manipulating instances of a type
from the type's internal representation, so that the representation can be
changed without changing the code that uses the type.

The next step beyond this is generic programming, where you have multiple
algebras that implement the same theory, and you write generic code that
uses the types and operations from the theory. Any algebra that implements
the theory can be plugged in, so the same code works with different types
and algebras without being changed.

In order to make generic programming work, there must be a way of selecting
which algebra to use. ADA uses two approaches.

The first is using explicit algebra parameters. A function maps an algebra
to either another function (a generic function), or to a module containing
functions (a generic module). This approach is general enough to work even
when there are multiple algebras over the same type that implement a given
theory, but it's also verbose.

The second approach is less general, but it supports run-time polymorphism.
The types of the arguments to a function are used to dynamically select an
algebra. This mechanism supports overloaded operators and heterogenous lists.
Operations that don't have any of the types in their domain can't be overloaded.
In order to support efficient run-time type dispatching, the types are
restricted to Data Types.

Universal Data Abstraction?
---------------------------
In Smalltalk, it appears that the only important thing about an object is the
message protocol that it accepts. Any object can be implemented to support
any message protocol. So it would seem that all functions are automatically
generic, since the requirements a function places on an argument is never a
specific internal data representation and a specific internal implementation,
but it is only that the argument supports a particular message protocol.

Is this "universal data abstraction" property a real thing that Curv can
implement, or what are the limitations? Can we automatically guarantee that
all functions are generic, without writing the functions in a special way?

My initial reaction is that this is an illusion. If all algebras were
single typed, and all operations in an algebra were functions that take
a single instance of type T as an argument (single dispatch: no constants,
no constructors, no binary operators), then this property would be achievable.
But I think it is unachieveable for arbitrary algebras. Smalltalk is selling
a feature that it doesn't actually deliver, and the cost is that you can't
write modular code.

---------------------------------------------------------------------
OOP has run-time polymorphism, but is anti-modular. The SML module system
is modular, but doesn't support overloading or run-time polymorphism. Other
systems lie in between. Haskell type classes support overloading, are more
modular than OOP and less modular than SML, and don't support run-time
polymorphism.

Informally, a type is a set of values plus a set of operations on those values.

We can formalize this. In Curv, all data is represented by immutable values,
and all operations are represented by pure functions. Thus, we can represent
informal types as Algebras. An Algebra is a set of values, a set of
constructors and operations on those values, and a set of axioms (or laws)
which fully specify the semantics of the operations.

Data abstraction is when you hide or ignore the representation of the values
in an algebra's value set, and write code solely in terms of the operations.
The axioms are the contract that you write the code against.

If we ignore the representation of the values in an algebra, and we ignore
the implementation of the operations, then what's left is the signatures of
the constructors and operations, and the axioms. This is a Theory, and it
can be thought of as the type of an Algebra. It turns out that multiple
Algebras can implement the same Theory.

Generic programming is when you implement generic algorithms over
abstract data types (algebras), using only operations provided by the
Algebra's theory. Then the algorithm is generic across multiple data types.

Popular programming languages provide many different _mechanisms_ that can
be used to implement abstract data types and generic algorithms.
For example,
  * Class based Object Oriented Programming with single dispatch
    and multiple inheritance (implementation inheritance, and protocol
    or interface inheritance). Objects contain both data and operations.
  * The Common Lisp Object System (CLOS), with generic functions that
    support multiple dispatch using a global dispatch table.
  * Haskell Type Classes. A type class is essentially a theory, although you
    can't formally specify the axioms. A type class instance is essentially
    an algebra. Type class instances permit only one implementation of a theory
    per data type.

These mechanisms can be quite complicated.
But Curv is a very high level language: simple and abstract.
I want to abstract over these data abstraction mechanisms, and provide
a high level abstract notation for data abstraction and generic programming.
Ideally, this notation should hide which data abstraction mechanism is being
used.

Values
------
All data is represented by immutable values. There are no mutable objects.
All operations on data are represented by pure functions (no side effects).
All API objects (functions, types, modules, etc) are represented by values.
All abstraction over API objects is done using ordinary functions.

Named API Values
----------------
API values (functions, types, modules, etc) can be given a name, which becomes
part of the value's identity.

Modules
-------
A module is a record with axioms. At present, we don't formally represent
the axioms, they are part of the documentation. But two modules are not
equivalent unless they have the same axioms. So that we have a way to
determine equivalence, the name of a module is part of its identity,
and we use a form of nominal equivalence for modules. Module members can
themselves be named values, so the indication of which members are named values
is another kind of metadata carried by a module.

I might want to use the word 'module' in a more general sense to mean
labelled or named value, or API value. In which case this section is
about record modules.

Types
-----
A type is a set of related values. They are related because they are used by
a common set of operations: they are part of the domain and/or range of these
operations.

Data Types
----------
A Data Type is a specific kind of Type associated with a value's type tag.
Every value has a unique Data Type associated with it (see [[Data Types]]).
In the case of user defined data types, defined using a simple 'data'
definition, the data type is synonymous with the data constructor name.

Algebras
--------
An Algebra is a module containing one or more named types, plus one or more
operations over those types.

An Algebra also has a set of axioms that describe its semantics, but for now
that will be represented by documentation.

The type names within an Algebra can be new data types defined locally by that
algebra using data constructor definitions, or they can be aliases for types
defined in other modules.

Syntactically, an Algebra is just a module with a particular structure.
No extra syntax is needed to construct a bare algebra.
We do need some syntax relating to theories:

 1. Declare that an algebra (aka a module) implements a theory (aka a module
    type). The significance is:
     * Checking (verify that the algebra contains the members required by
       the theory).
     * Type membership. A Theory is a module type. Given this declaration,
       a function with a theory-typed parameter will accept this algebra
       as an argument.
     * Default values. It is proposed that a Theory can provide default
       values for operations. The rationale is to allow a Theory to be
       extended with new operations without breaking existing algebras.
       (In this extension/backward compatibility scenario, the algebra
       wouldn't directly reference a default value.)

    Possible syntax for implementing a theory:
     * 'implement <theory>' in a module body.
       This would allow other members to reference default values, so the
       existence of the defaults would become part of the theory's contract.
       This is on the edge of reinventing the contracts between OOP classes
       and subclasses, which are a misfeature of OOP.
       Is this contract extension undesirable feature creep?
       Referencing defaults is not a requirement right now.
     * '<theory>(<module>)' -- Using a <theory> value, which is a type,
       as a constructor for values of that type. This is a common Curv idiom.
       This constructs a new algebra value that is possibly extended with new
       members possessing default values. This would allow you to use someone
       else's algebra to implement your new theory. This seems more composable.
       There's no way for the module to reference members 'inherited' from the
       theory, so there are no OOP implementation inheritance semantics (good).
       The syntax seems parsimonious: no new 'implement' keyword.
     * 'algebra <theory> <module>' -- Like above, except we don't require
       Theory values to implement the Function theory. Sometimes I'm too
       quick to overload various types to use function call syntax as a
       convenient way to invoke some operation.

 2. Add the methods in an algebra to the overloaded virtual operations of a
    theory that the algebra implements. This is an extended form of #1 that
    requires some additional restrictions for implementability.
    This operation can be performed either in the module where the algebra
    is defined, or in the module where the theory is defined. Possible syntax:

        overload <theory>(<algebra>)

Theories
--------
A Theory is like the type or signature of an Algebra. A Theory contains:
* A set of type names.
* A set of operation signatures (name/type pairs).
  The signatures are written in terms of the types from the first clause.
* A set of axioms over the operations. For now, these are encoded as comments,
  but later I'd like to have formal axioms that can be used to generate unit
  tests, and which can be used by an EGG based code optimizer.

A Theory is also a dependent record type: the operation signatures depend
on the types.

The operation signatures are used to implement
dispatch of overloaded operators (see Abstract Values).

A Theory can inherit from other Theories (multiple inheritance).

There is a predicate for testing if an Algebra implements a Theory.
In the case of Theory inheritance, the Algebra is also considered to implement
the super-theories of the Theory it is declared to implement.

There is a predicate for testing if an abstract value implements a Theory.

An Algebra can be declared to implement a Theory, and some checking is done.
If a formal parameter is to be bound to an Algebra, you can specify
a Theory as a parameter type. So this gives us checked Algebra parameters,
needed for the Better Error Messages feature.

Generic Programming
-------------------
I see two approaches to generic programming.

* Extrinsic: The mapping from the value (containing the representation)
  to the algebra (containing the operations) is outside of the value.
  This means that multiple implementations of same Theory can be associated
  with a value. Eg, a Number can be part of a Monoid in multiple ways.
  While this seems more flexible, it doesn't support heterogenous lists.
  Requires more boilerplate and bureaucracy, and the use cases are less common.

* Intrinsic: The mapping from the value to the algebra is internal to the value.
  This means you can have overloaded operators that choose the right
  implementation, with no boilerplate or bureaucracy. Very convenient.
  Plus heterogenous lists. We would use this approach for Shapes.

Algebra Passing Style
---------------------
This is a variety of extrinsic generic programming.

You can write generic algorithms on simple data using local semantics, if you
explicitly pass algebras around as arguments, using "Scrap Your Typeclasses"
(value level) or "SML Modules" (module level).
 * No additional language features are needed to support this.
 * Is verbose, boilerplatey.
 * Doesn't support heterogenous lists of generic values.

Intrinsic Generic Programming
-----------------------------
This is the more popular and important kind.
Dynamic languages provide multiple implementations of this, by exposing
complex and arbitrary *mechanisms* (more complexity than is needed to solve
the problem).
 * class-based single dispatch OOP with multiple inheritance
 * SELF prototypes and delegation
 * Javascript prototypes
 * CLOS classes and generic functions with multiple dispatch
I'd rather focus on the requirements and hide the mechanisms behind simple,
high level syntax. Try to use data abstraction and algebra driven design
to simplify intrinsic generic programming.

Requirements:
* Abstract syntax for invoking an overloaded operation.
  It better be function call syntax 'f(x)', rather than 'x.f()' or the like.
  We want to write simple generic code and hide the implementation details
  of how intrinsic operations are dispatched.
* Supports binary operators, and more generally, supports multiple dispatch
  within a single-sorted algebra.
* Optional: multiple-dispatch for multi-sorted algebras (more than one carrier
  set in the algebra).
* Supports heterogenous lists.
* You can add a new constructor to an existing Theory, without modifying the
  Theory. Eg, add a new Shape type.
* Pattern matching over the constructors that implement a Theory. Since this
  set of constructors is freely extensible, there has to be a default clause.
* You can add a new Theory to an existing constructor, without modifying the
  constructor. Optional? Solves the Expression Problem. Eg, define a SVG
  exporter for Shapes with different code for different shape constructors.
  It could be a function that switches on shape constructors. Or it could be
  a more distributed design, where the author of a new shape constructor can
  implement an SVG_Export method.
* Given a value, query the theories that it supports.
  Could be used by the IDE for suggesting completions to a function call,
  in the context of 'shape >> ?'.

An implementation of abstract values with intrinsic algebras
------------------------------------------------------------
Abstract values are labelled values (they contain a constructor name).

Theories (participating in this scheme) are labelled values.

Constructor and theory names are used to look up algebras when resolving
an overloaded operator. See below.

For modularity, an overload declaration linking a constructor to an algebra
is only permitted in the module defining the constructor, or in the module
defining the theory.

Which means we need to define what module identity is.
* A 20th century idea: there is no internet, just your local machine.
  Constructing a module causes the side effect of allocating a module id
  unique to the current address space. Not referentially transparent.
  Doesn't work if you have functors (functions that generate modules),
  because you need F x and F y to be the *same* module if x==y.
* An earlier idea: named modules have a file or URL at the root of their
  name path. This makes the name globally unique. It also separates Curv into
  an "inner" and "outer" language, in which named modules, constructors and
  algebras are only defined in the outer language.
* A modern idea (git, IPFS, Unison): a module identity is a hash.
  Two copies of the same module, with the same source code, have the same
  hash, so they are identical. This is referentially transparent, and
  it does not split Curv into an inner and outer language. Two versions
  of a module with different code will have distinct identities. There
  is no merging of module identity based on semantic versioning. Two
  copies of the identical library code obtained from different URLs
  will be equivalent, thus we don't depend on URL identity to determine
  value and module identity. This also works for function identity. A
  full description of a labelled value (that describes identity) needs
  to include a hash. Value descriptions containing these hashes can be
  transmitted across a network while preserving full value semantics.

Given an overloaded operator call 'T.f x', we need to use the constructor of x
and the theory T to look up an algebra A of type T, so we can call 'A.f x'
* Case 1: the constructor is part of the algebra A.
  The overloading happens in the module the algebra is defined.
  Alternate wording: constructors and algebra are defined in same module.
* Case 2: the constructors are external to the algebra.
  The overloading happens in the module the theory is defined.
If the algebra and theory modules are mutually recursive, then conflicting
overload definitions are possible, which must be reported as an error.
This can only happen inside a single package, as the package graph is acyclic.
The important thing is to make impossible conflicts between packages.

In case 1, the mapping could be stored in the constructor/value.
In case 2, the mapping could be stored in the theory.
This has local semantics. There is no requirement for a global dispatch table
(except maybe as a cache for performance reasons).

Theory Syntax
-------------
A Theory consists of:
* zero or more supertheories (maybe those are declared externally)
* one or more type names (of 'member types')
* zero or more member signatures
* axioms (not supported yet, must be described in documentation)

A Theory is normally a named value.
Is there even a use case for anonymous theories? (Obviously I'm thinking about
orthogonality.) When would I pass an anonymous theory literal as a function
argument? (Answer: some weird higher order programming, like theory or algebra
combinators, that I haven't considered yet.) What operations can be performed
on an anonymous theory? (How do you build complex theories or algebras out
of simpler ones?) Theories are values, so you *can* pass them as arguments etc.

A 'member signature' is an operation, a constructor, or a constant.
* An operation is a function that takes at least one argument of a member type.
* A constructor is a function whose result is a member type, or a constant
  of a member type.
* A constant is a member whose signature doesn't contain a member type.
  It is a per-algebra constant, similar to a static member of a C++ class,
  or a class variable in Smalltalk.

A member signature is, syntactically, 'name : signature',
where 'signature' is domain-specific syntax for a type expression,
including some handy abbreviations. Signatures are a distinct phrase type;
they are not expressions, just like patterns are not expressions.
[Rationale: need to analyse signatures at compile time, find all occurrences
of member types.]

Signature syntax:
 * type names and type constructor expressions from the Types proposal
 * [type1,type2,type3] -- a tuple type
 * {name1: type1, name2: type2} -- a record type
 * Sig1 Sig2 -> SigResult

So, like:
    def Shape = theory (T) {
        distance: Vec3 T -> Num,
        colour: Vec3 T -> Num,
        bounds: T -> [Vec3,Vec3],
        is_2d: T -> Bool,
        is_3d: T -> Bool,
    }
or:
    def Monoid = theory (T) {
        zero: T;
        op: [T,T] -> T;
    }

A theory member can be a lens, and this is how we construct theories for
container values that have mutable elements. It is better if the syntax
for lens fetch is <lens> <x>, i.e. function call notation. This makes the
theory interface more abstract. You can promote an element selector from
a function to a lens without breaking existing code.

Operations on Theories:
* Container for overloaded operations. Given a shape S,
    S >> Shape.distance[x,y,z]
* <theory>.<typename> -- a run-time polymorphic classifier for a value whose
  type is an instance of a member type.
* <theory>.<operation> -- a run-time polymorphic, overloaded operation.
* <theory>.<constructor> -- illegal, can't be resolved
* <theory>.<constant> -- illegal, can't be resolved
* Referenced by an Algebra.

I think that a theory is really a dependent module type.
A theory with zero member types is just a simple module type (an interface).

If supertheories are declared externally, then they can be declared either
beside the subtheory, or beside the supertheory.

More features:
* Theory constants (same for all algebras), which can implement additional
  'static' operations based on the 'virtual' operations that are implemented
  on a per-algebra basis. Needed because a Theory is also a module. For the
  purpose of providing an abstract module interface to a Theory, we don't
  expose which generic operations are 'static' and which are 'virtual'.
  In fact, this status could change without changing the public interface.
* Default values for virtual operations. This is useful because it allows a
  new virtual operation to be added without breaking existing algebras that
  implement the theory.
* Maybe: a pragma to suggest that a virtual operation like T->X (where T
  is a member type) should be cached in the value as a data field.
  Eg, in Shape, bounds, is_2d and is_3d could be declared data fields.
  This would be a performance optimization. It won't work for algebras that
  implement operations over external constructors. But it will work for
  algebras that define their own constructors.

Note: Theories seem as complex and ad-hoc as C++ classes. Their dual nature
(they are both modules and module types) is driving this. What about my
orthogonality principle? I feel like I just accidently reinvented C++,
that maybe my C++ background has biased the design towards complexity.

We can separate the module and type aspects of theories. A theory is not a
module, but a module can include a theory and get the type and operator names.
So there is both a Shape module and a Shape theory:
    Shape = {
        theory = make_theory (type) {
            distance: Vec3 type -> Num;
            ...
        };
        include theory;
        // non-virtual shape operation
        size S = let b = Shape.bounds S in b.[MAX] - b.[MIN];
    }
Use naming conventions to reduce boilerplate, so a module containing a
'type' field behaves like a type. Then 'Shape' is the type of a shape and
'Shape.theory' is the type of a shape algebra. Another naming convention:
maybe a module delegates to its 'theory' field, if that field contains a
Theory. That eliminates the 'include theory;' boilerplate.

`Theory` is the type of a theory value.

Algebra Syntax
--------------
An Algebra is a module with:
* Implements a Theory. Must define fields specified by the Theory with matching
  types or signatures.
* Can optionally be 'overloaded'. This is a kind of definition.

Shapes
------
There are many shape constructors supporting the same generic operations,
so we need a Shape Theory, and we need to choose a method of binding an
algebra to a shape value that supports heterogenous lists.
Shapes are abstract values. A shape value can be identified as such
in a purely local way, by examining the value.

'cube n = box [n,n,n]' is simple and works.
What if I upgrade 'cube' to a labelled constructor, with extra properties
due to 'cube' being a subtype of 'box' with extra symmetries?
