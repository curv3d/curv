Fast Evaluator
==============
Here are the possibilities for fast expression evaluation:
* Tail-threaded interpreter. Fastest known interpreter implementation, given
  current CPU architectures and recent features of clang/gcc.
  Should work on all platforms (except WASM?) when using clang.
  Hopefully I can use this for partial evaluation as well.
* Internal JIT: generate machine code into local memory, then execute it.
  Forbidden in iOS (Zee wanted to do an iOS port). Impossible in WASM (per Wasm3
  readme). Complicated, as I need to generate low level IR for the JIT library.
* External JIT: generate machine code into external storage (a file, or an
  external WASM module), then execute it.
  * Generate C++, compile to *.so or *.dll, dlopen and execute. Forbidden in
    iOS. Native only: Not workable in WASM/web browser.
  * WASM/JIT: Generate an external WASM module and then execute it.
    * https://github.com/indutny/wasm-jit/
    * JIT based x86 emulator in WASM: leaningtech.com/cheerpx
  * LLVM IR to native code, WASM or SPIR-V? In 2021, SPIR-V is not in LLVM.
    Khronos has a repo, WIP, focussed on OpenCL SPIR not Vulkan SPIR-V.
  * MLIR has LLVM & SPIR-V/Vulkan backends. github.com/google/IREE?
    Distinct spv and llvm IR dialects, not a single IR with multiple targets.
  * SPIR-V -> LLVM translator in MLIR.
  * .NET/Mono CIL (an IR). Linux/MacOS/Windows/Android, profiling JIT.
    Java like VM, OOP, stack machine, generational GC. Optional LLVM backend.
    AOT compile to LLVM bitcode, then WASM.
* GPU compute. Super fast for appropriate work loads. Strangely, this is the
  only universal option for optimized native code execution.
* AOT compiler. Generate C++, compile to native code or WASM.

SimpleJIT:
* SubCurv compiles to statically typed optimized code, only on request.
  Compilation is slow, evaluation is fast. Generate code as a self-contained
  module. Small static data is generated as literals, large static data is
  passed as arguments (like uniform variables). This design works for GPU,
  WASM (generate external WASM module), any kind of JIT, any platform.
  * Use LLVM to generate native code, SPIR-V, WASM. Maybe via libclang.
* For full Curv, transliterate each VM instruction into machine code while
  keeping state in the interpreter data structures. Code generation is fast
  enough for live programming. Run-time introspection: debugger works on
  interpreter data structures. Tail call not a problem.
* Use interpreter on platforms where machine code JIT not available.
* Eg, https://blog.erlang.org/a-first-look-at-the-jit/
  Uses <asmjit.com>: small fast C++ lib, ISA dependent API.

Use cases:
* Partial evaluator; low latency live programming: tail-threaded interpreter.
* Mesh export:
  * Interpreter: too slow.
  * Internal JIT: not iOS or WASM.
  * External JIT: not iOS. On WASM, jit to WASM, could be 70% of native.
  * GPU: fastest and most universal, if I can code the algorithms.
* Implement Curv compiler and runtime in Curv: AOT compiler.

Priorities:
 0. External JIT. Required for GPU, needed for mesh export until I can
    implement a GPU version.
 1. GPU compute language.
 2. Tail-threaded interpreter. Faster, but finicky & bleeding edge.
    Needs a fallback for WASM.
 3. AOT compiler, offshoot of current GLSL/C++ compiler.
 4. Internal JIT. Difficulty + lack of universality makes this lowest priority.

Two Strategies: High-Perf vs Run-Everywhere
-------------------------------------------
1. High performance: native only, advanced GPU >WebGPU, Dali graphics
   tail-threaded interpreter, full Curv JIT like Julia.
   Advanced technology, not held back by web (or iOS) limitations.
2. Run-Everywhere: runs native or in a web browser
   old-style interpreter, SubCurv-only JIT.

Tail-threaded interpreters
--------------------------
New technique, depends on -O2 tail call optimization in clang and gcc,
made reliable by [[clang::musttail]] attribute in Mar 2021 clang (LLVM 13).

Will Rust get this? rust-lang/rust/issues/217 closed wontfix in 2013.
There is a debate in /r/rust "Are we finally about to gain guaranteed Tail
Calls in Rust?" 25-apr-2021 which proposes to reopen the issue. It will take
a while (years?) for this to land in Rust stable, but the same could be said
for landing in Ubuntu LTS's packaged version of clang.

"Parsing Protobuf at 2+GB/s: How I Learned To Love Tail Calls in C"
blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html

This technique lets you write interpreters that are as fast as the
assembly language LuaJIT interpreter, but in C, with a more modular and
extensible code structure. Relies on a non-standard language extension,
restricted to clang and possibly gcc, not MSVC, and probably not portable
to niche legacy architectures. However intel and ARM are just fine.
The Windows calling convention creates limitations, see Wasm3/M3 doc below.

Per Wasm3: 4-15x slower than native on modern x86-64. Older CPUs are slower.

The technique is finicky and you have to look at generated assembly code
to ensure you didn't screw up the magic rituals needed to make the compiler
generate efficient code. So this probably needs more time to bake.

See also the Wasm3 interpreter architecture (aka M3, Massey Meta Machine):
github.com/wasm3/wasm3/blob/main/docs/interpreter.md

Small hackable example (283 lines of C++):
gist.github.com/snej/9ba59d90689843b22dc5be2730ef0d49
lobste.rs/s/wmuvxw/tails_tiny_forth_core_written_as_hack_for

I think this technique is presently impossible in WASM, based on
blog posts written by the CheerpX team. It requires the WASM tail call
feature, which is stalled in the implementation phase for 2 years
(Mozilla hasn't implemented it).
 * https://medium.com/leaningtech/extreme-webassembly-1-pushing-browsers-to-their-absolute-limits-56a393435323
 * https://medium.com/leaningtech/extreme-webassembly-2-the-sad-state-of-webassembly-tail-calls-f5d48ef82a87

Because of this, the C/C++ code needs to be written so that it "falls back"
to generated code that works correctly without the tail call optimization,
and doesn't blow up the call stack. The interpreter will be slower without
the tail call optimization, probably comparable to the Curv 0.4 interpreter.

Another threaded interpreter approach:
  sfkleach December 13, 2018, 4:20pm

  In my own threaded interpreter, built on top of gcc, I directly plant
  the labels into the generated code-array, so that the dispatch is “goto
  *pc++;” where pc is a pointer into the code. This is much more direct
  than “goto *dispatch_table[ code[ pc++ ] ]” where pc is an int that
  is indirected through a code array. I designed the instruction set so I
  could actually try out both forms and my experiments showed a very large
  difference in performance for gcc; the exact value depends on architecture
  but never less than a factor of 2.5 and up to a factor of 6.
But tail-threaded is more modular.

JIT
---
Can I JIT the entire language to native code, or is this restricted to SubCurv?
* Proper tail calls. LLVM supports this for x86 and WASM, not ARM (? per docs).
  No tail call support in WASM (Firefox) and Cranelift.
  A solution is to generate weird code, eg use heap allocated stack frames.
  https://people.cs.uchicago.edu/~jhr/papers/2020/ifl-smlnj-llvm.pdf
* All code + data structures in the Curv runtime (the ABI) can be described
  using platform-independent C. Anything that consumes or produces C-describable
  code and memory structures can interoperate with Curv, including all
  popular JIT libraries, WASM, systems programming languages, languages with
  a C FFI. In the general case, we may take a 50% performance hit over an
  optimal representation. SubCurv is as fast as C.

Here is a list of JIT libraries in 2021: https://github.com/vnmakarov/mir
* LLVM and libgccjit are huge and slow, produce fast code.
  Bad for interactive/live coding. Good for CPU mesh generation.
* asmjit.com is small 300kB, C++, used in production by some serious projects.
  Targets Intel (experimental ARM port). API is somewhat arch. dependent.
* Cranelift has institutional support (used in firefox, rustc).
  70K LOC Rust. The use of Rust makes this a significant dependency. (~1.5MB)
  (Although, the generator module has minimal dependencies and a nostdlib
  option.) The institutional support means this will work on all Rust and
  Firefox platforms, will have vector ops, will have a community.
  No tail recursion (stalled for 2 yrs) but good for SubCurv.
  Maybe someone will support a C++ shim layer.
* The hobby projects are interesting, and prove the idea that you can get
  70% of LLVM performance with 10% of the code. But, more limited platform
  support, potential limitations like no vector ops, etc.
  * MIR looks cool (tiny 10K LOC C, fast, competitive performance)
    But (2021/Feb) is unstable and unreleased. A hobby project.
  * ... see mir readme for more

What is my JIT story for Curv running in WASM?
* See CheerpX, referenced above, a sophisticated JIT running in WASM.
* Maybe there is no JIT under WASM: interpreter only.
* JITing to WASM makes sense for SubCurv. But WASM lacks proper tail recursion.
  The tail recursion proposal entered Phase 3 (implementation) in Aug 2019.
  Chrome has an implementation, it has been stalled in Firefox and Cranelift
  for 2 years.
* Maybe there is a portable JIT library that generates native code in a
  native executable, or WASM code in a WASM executable. For JIT, I need
  the ability to execute a WASM module in my own address space, not in a
  separate address space. Is this possible? It should be cheap for the
  WASM module to refer to values and compiled functions in the local address
  space, allocate memory using the global allocator, modify data structures
  in the local address space, and return results, all without the expense
  of copying code and data to and from a remote address space.
  * LLVM has a WASM target.
  * Maybe Cranelift will get a WASM target for Rust?
  * Wasm3/readme says WASM internal JIT is not possible: "you can't generate
    executable code pages in runtime".
* If I can't find a JIT library with native and WASM targets, then I need
  to generate WASM directly.
* A JIT library takes IR as input. Maybe I can use WASM as my JIT IR input
  language. Maybe I use Wasmer as middleware on native platforms to select
  between cranelift and llvm code gen?
  * How do I tunnel native function and data pointers through WASM and Wasmer
    into the native code that Wasmer generates? This may be impossible, given
    the sandboxing guarantees.
  * This is not light weight: LLVM is 13MB, vs 1.5MB for Cranelift.
    LLVM alone is 13MB, vs 1.5MB for Cranelift. Curv is currently 7MB stripped.
    https://wasmer.io/posts/wasmer-python-embedding-1.0
  * WASM IR will be slower than using Cranelift directly in a native app.
* Maybe Curv is only shipped as a WASM executable. Then my JIT targets only
  WASM as its native code. I could ship native Curv as a native wrapper
  around a WASM exe. Later, there might be an ecosystem for shipping WASM
  apps directly.
* Can I find prior art for JIT languages that run under WASM?

-----------------------------------------------------------------------------
## OLD: Which JIT library to use?
LLVM is the dominant JIT library. But it has a serious problem:
compilation is too slow for interactive use. Also, there are
bugs and limitations that aren't immediately apparent. LLVM isn't
really that good for JIT, which creates a chicken-and-egg problem:
most people don't use it, so the problems get fixed very slowly.

Most serious language projects use a custom machine code JIT compiler.
In the Python world, the Unladen Swallow project (using LLVM) failed,
and was superceded by PyPy, which uses a custom machine code generator.

There is perhaps an open niche for a really good JIT library,
and the mythology around LLVM perhaps prevents a serious effort at
creating this library (because: why not just improve LLVM instead?)
The answer may be that the LLVM architecture is incompatible with
fast, interactive-grade compilation.

LLVM is the dominant standard, used by Haskell, Swift, Erlang, others.
Unequaled power and flexibility.
Fantastically complex, documentation difficult to navigate.
Memory management is problematic:
* All allocated objects are owned by an LLVMContext instance?
* In practice, I need a single global LLVMContext shared by all scripts
  compiled by a Curv session, and this thing will just keep growing.
Two JIT interfaces:
* Legacy JIT interface used by the tutorial. Custom instruction encoder.
  Allegedly not supported by LLVM 3.7.1, not sure when it went away.
  * LLVM 3.4: support for exception handling has been removed from old JIT.
    Use MCJIT for EH support.
* Newer MCJIT interface. Uses MC to encode instructions. Needs to perform
  runtime linking of object files (it emits ELF object files into memory).
  Less JITy (once a module is compiled, it's done, you can't update it further).
  * oct2014: mcjit doesn't support exception catching on windows, that would
    require COFF support.
  * The current design for "stack trace inside of an exception" relies on
    low cost exception catching (which is more expensive on windows).
    Instead of throwing an exception, I could return an exception value,
    and test for a tag value in the Value or pointer returned by a function.
  * IR to machine code in MCJIT is slow (implied by forum post).
    Open Shading Language still uses LLVM 3.4, because 50% slower JIT time is
    a big deal for human interaction.

LibLightning -- http://www.gnu.org/software/lightning/
Very simple and fast JIT library.
However, I don't see any way to call C functions from JIT generated code.
Let alone catch exceptions raised by C functions.

LibGccJit --
Project to refactor GCC code into a backend JIT library.
In gcc trunk as of gcc 5. Alpha quality code with unstable API.
It uses context objects, but there's documentation, so the lifetimes
of objects are clear. The generated machine code is independent of the
context object, which can be freed before running the machine code.
May 2016: author has just learned about need for tail call support
(LLVM had this years ago, albeit intel only).

LibTcc --
World's smallest and fastest C compiler. There is no parse tree: the parser
outputs machine code directly into memory. Lovely. The only data structure
you can feed it is C source, since there is no IR. Easy to use.
Supports x86, x86_64, ARM. No optimization, obviously.

LibJit -- gnu.org/software/libjit
Created in 2004, barely updated or used any more.

== How To Use LLVM?

Where are the LLVM module boundaries in Curv?
Let's assume no optimization across module boundaries.
* each expression/definition typed interactively is a module
* each script file is a module

So we compile a command line expression/script file to an LLVM module,
then evaluate it.

== Optimized Machine Code
If I were compiling Curv to machine code,
I'd use some optimizations that aren't really practical in the byte code VM.

* **Registers.** Efficient expressions, which store temporaries in registers,
  rather than on the stack, where feasible. An efficient calling convention,
  that stores arguments in registers, rather than on the stack, where feasible.
  (Note that Curv doesn't support varargs for this reason.)
* **Lazy Boxing.**
  Keep values their unboxed representation where possible.
  Convert to boxed representation only when necessary.
  In an Expression node, I would keep track of the compile time type of
  the expression as a bitmask of the 8 basic types.
* **Lazy Ownership.**
  I would be lazy about incrementing reference counts.
  If I am copying a curv::Value from another storage location,
  I don't automatically increment the reference count.
  Instead, I just mark the Expression node as "borrowed";
  "owned" would mean we own the reference.
  * The function calling convention assumes that reference parameters
    are borrowed. The caller is responsible for releasing argument references,
    as necessary. This is no good for tail call optimization.
    So, we only optimize *recursive* tail calls. A self-recursive tail call
    turns into a branch to the beginning of the function, after updating
    the parameter values. A mutually recursive tail call to another function
    G involves inline-expanding the body of G into the current function.
    There's a top level loop that selects which function to enter on each
    iteration. This approach to tail-call optimization has other benefits:
    * It's compatible with the C calling convention, which may be good for
      interoperability with C.
    * LLVM doesn't support tail-recursion optimization on ARM.
  * The "arguments are borrowed" calling convention prevents
    the `use_count==1` optimization for destructively updating data structures
    like arrays.
    * I could use the more efficient "borrowed arguments" calling convention
      for unboxed function calls (to a function known at compile time).
      For specific arguments that require the caller to know the use_count,
      we provide an accurate use count. For the general "boxed" calling
      convention, we use the "owned arguments" calling convention.

These optimizations make the code run faster, but they make introspection
of the VM state more difficult (for debugging and stack unwinding).
So you need a map from instruction positions to stack & register state.
This map is expensive to construct, which slows down the compile enough
to be undesirable for interactive use. So, the VM needs to provide a choice
between fast compile and "adequate" run time performance (10-20x slower than C),
vs slow compile and fast run time. There should be an ahead-of-time compile
tool that compiles a top-level script into a shared object/DLL.
